FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04

# 1. Установим базовые утилиты и Python
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    unzip \
    python3.10 \
    python3.10-venv \
    python3-pip \
    build-essential \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    && ln -sf python3.10 /usr/bin/python

# 2. Обновим pip и установим torch
RUN pip install --upgrade pip

# PyTorch + CUDA (замени версию под нужную тебе)
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. Установим необходимые Python-пакеты
RUN pip install \
    transformers==4.51.3 \
    peft==0.15.2 \
    accelerate==1.6.0 \
    unsloth==2025.4.7 \
    sentencepiece==0.2.0 \
    bitsandbytes \
    matplotlib \
    scipy \
    scikit-learn \
    opencv-python \
    Pillow \
    notebook \
    jupyterlab 


# !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#!pip install --no-deps unsloth
#RUN pip install -U \
#    unsloth \
#    unsloth_zoo \
#    protobuf \
#    datasets \
#    xformers==0.0.29.post3 \
#    cut_cross_entropy \
#    trl==0.15.2 \
#    triton \
#    accelerate \
#    bitsandbytes \
#    peft \
#    huggingface_hub \
#    hf_transfer \
#    sentencepiece 




# 4. (опционально) установка GGUF тулов, если будут поддерживаться
# RUN pip install llama-cpp-python

# 5. Рабочая директория
WORKDIR /qwen



COPY qwen /qwen
RUN pip install -r requirements.txt
# 6. Копирование кода (если нужно)
# 7. Команда по умолчанию
#CMD ["python","main.py"]
#CMD ["jupyter","lab", "--ip=0.0.0.0","--port=8888", "--no-browser", "--allow-root"]
#CMD ["bash", "-c", "python3 main.py & jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='bigdaddy'"]
CMD bash -c "jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='bigdaddy' & python main.py"

#CMD ["tail", "-f", "/dev/null"]
