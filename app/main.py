import gradio as gr
import base64
import requests
import time
from io import BytesIO
from PIL import Image
import os
from corrector import PhraseCorrectorNgrams, correct_text
import psutil
import subprocess

#OLLAMA_HOST = "http://ollama:11434"
OLLAMA_CHAT_URL = os.environ.get("OLLAMA_BASE_URL")# f"{OLLAMA_HOST}/api/chat"
OLLAMA_TAGS_URL = os.environ.get("OLLAMA_MODEL_URL")#f"{OLLAMA_HOST}/api/tags"

SYSTEM_PROMPT = '''
–í—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç OCR , –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å —Ä—É—Å—Å–∫–∏—Ö —Ç–æ–≤–∞—Ä–Ω—ã—Ö —ç—Ç–∏–∫–µ—Ç–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –í —Ç–µ–∫—Å—Ç–µ –º–æ–≥—É—Ç –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º.
'''

USER_PROMPT = '''
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏–∑–≤–ª–µ–∫–∏—Ç–µ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. 
'''

corrector = PhraseCorrectorNgrams(
    words_file="/data/words.txt",
    score_cutoff=80,
    min_len=3,
    max_ngram=3
)


def get_system_info():
    ram = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=0.5)
    ram_info = f"üß† RAM: {ram.used // (1024**2)} MiB / {ram.total // (1024**2)} MiB"
    cpu_info = f"‚öôÔ∏è CPU: {cpu}%"

    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        used, total = result.stdout.strip().split(', ')
        gpu_info = f"üñ•Ô∏è GPU: {used} MiB / {total} MiB"
    except:
        gpu_info = "üñ•Ô∏è GPU: –ù–µ –Ω–∞–π–¥–µ–Ω–æ"

    return f"{cpu_info}\n{ram_info}\n{gpu_info}"


def get_available_models():
    try:
        response = requests.get(OLLAMA_TAGS_URL)
        response.raise_for_status()
        tags = response.json().get("models", [])
        return [model["name"] for model in tags]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        return ["gemma3:1b"]  # –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏


def encode_image(image: Image.Image) -> str:
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def chat_with_ollama(system_prompt, user_prompt, image, selected_model, temperator=0.2, correct_text_flag=False):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    if image:
        image_base64 = encode_image(image)
        messages[1]["images"] = [image_base64]

    ollama_args = {
        "model": selected_model,
        "messages": messages,
        "stream": False,
        "temperature": temperator,
        "max_tokens": 200
    }

    result = ""
    start_time = time.time()  # Start timing
    response = requests.post(OLLAMA_CHAT_URL, json=ollama_args)
    elapsed_time = time.time() - start_time  # Calculate elapsed time

    if response.status_code == 200:
        
        result = response.json()["message"]["content"]

        if correct_text_flag:

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ PhraseCorrectorNgrams

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            result, corrections_log = correct_text(result, corrector)
            print("–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:", result)  # Added print statement for corrected text
            print("–õ–æ–≥ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:", corrections_log)  # Added print statement for corrections log
            
    else:
        result =  f"–û—à–∏–±–∫–∞: {response.status_code}\n{response.text}\n\n"
    
    return result, f"{elapsed_time:.2f} —Å–µ–∫—É–Ω–¥"

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ Ollama –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
available_models = get_available_models()

with gr.Blocks() as demo:
    gr.Markdown("## –ß–∞—Ç —Å Ollama")
    sysinfo = gr.Markdown()

    with gr.Row():
        with gr.Column():
            system_prompt = gr.Textbox(label="–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", value=SYSTEM_PROMPT)
            user_prompt = gr.Textbox(label="–í–∞—à –∑–∞–ø—Ä–æ—Å", value=USER_PROMPT)
            image = gr.Image(type="pil", label="–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)")
            model = gr.Dropdown(choices=available_models, label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å")
            temperature = gr.Slider(minimum=0, maximum=1, step=0.05, value=0.2, label="Temperature")
            correct_text = gr.Checkbox(label="–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç?", value=False)
            btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")
        
        with gr.Column():
            result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
            duration = gr.Textbox(label="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    btn.click(
        fn=chat_with_ollama,
        inputs=[system_prompt, user_prompt, image, model, temperature, correct_text],
        outputs=[result, duration]
    )

    demo.load(get_system_info, outputs=sysinfo)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)